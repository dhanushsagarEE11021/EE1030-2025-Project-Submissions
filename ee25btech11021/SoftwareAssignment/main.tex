\documentclass[12pt,a4paper]{report}

\usepackage{graphicx}      % images
\usepackage{amsmath, amssymb} % math
\usepackage{geometry}      % margins
\usepackage{setspace}      % line spacing
\usepackage{hyperref}      % hyperlinks
\usepackage{float}         % to control figure placement
\usepackage{listings}      % for pseudocode/code
\usepackage{xcolor} 
\usepackage{lmodern} 
\usepackage{subcaption} 

% colors for code

\geometry{margin=1in}
\setstretch{1.3}

% --- Code style ---
\lstset{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    showstringspaces=false
}

% --- Title Page ---
\title{
    \vspace{2cm}
    \textbf{\Huge Project Report}\\[0.3cm]
    \textbf{\Large SVD-based Image Compression}\\[1.5cm]
    \textbf{Submitted by}\\[0.2cm]
    \textbf{Dhanush Sagar}\\
    Roll No: EE25BTECH11021\\[0.5cm]
    Under the guidance of\\
    \textbf{Prof. GVV Sharma}\\[2cm]
    Department of Electrical engineering\\
   
}
\author{}
\date{}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

% =======================================================
\chapter{Summary of Strangâ€™s Video}
In This lecture  Prof. Gilbert Strang explains how any matrix $A $ can be decomposed into three matrices:
\begin{align}
    A = U \Sigma V^T
\end{align}
where:
\begin{itemize}
    \item $U $: orthogonal matrix (left singular vectors)
    \item $\Sigma $: diagonal matrix with singular values
    \item $V $: orthogonal matrix (right singular vectors)
\end{itemize}

\textbf{Formate}\\
he initially started taking a matrix A ,and wanted to find a matrix V which contains orthonormal vectors $v_i$(of rowspace) such that result with each vector will give rise to $\alpha u_i$ which(u) are again orthonormal(u) therefore forming a orthonormal matrix .this was his basic idea.this can be written as $AV = U\Sigma$ where $\Sigma$ is diagonal matrix of alpha's.now since V is a orthogonal matrix multiplying $V^T$ both side will give eq(1.1).

\textbf{finding V and U}\\
for finding matrix we can just do $A^TA$ to get V and $AA^T$ to get U
we do these we get resultent as $V\sigma^2V^T$ and $U\sigma^2U^T$ .Seeing these squation and comparing with $QVQ^T$(informed in the begining of the class )we can say directly that V are eigen vectors of $A^TA$ and $\sigma^2$ are eigen values of of the same matrix and same thing goes from U .From these we derive matrices V ,U and also $\Sigma$.
\newpage
\textbf{Other important thing}|\\
sir also mentioned about one important thing about orthonormal vectors of V and U span the whole input space (for V its $R^n$), But for all vectors vectors doesnot contribute to the Matrix A,only vectors belonging to row space (V) and column space (U) of A contribute ,remaining vectors which are still orthonormal to input space but belongoing to nullspace of A does not contribute basically there $\alpha$ will be zero(interms of $A^TA$ there are the eigen vectors of 0 eigen value)  we can skip these vectors but we are including to make a squared full ranked orthogonal matrix.

\textbf{How i used this idea to compress the image}\\
now if we use all the singulur values and vectors we will get the exact image but intead of that if we will just use vectors and singular values that will contribute the majority of the image ,it will form the exact image with less clarity which would be fine.(basically we will find singular vectors corresponding to largest singular value which will contribute most)  this will the also decrease the rank of Matrix A (which depends on number of singular values u take )

\begin{align}
    A_k = U_k \Sigma_k V_k^T
\end{align}
% =======================================================
\chapter{Explanation of the Implemented Algorithm}
\section{Mathematical Background}

To compress the image, we retain only the top $k $ singular values:
\begin{align}
    A_k = U_k \Sigma_k V_k^T
\end{align}

where $U_k $ contains the first $k $ columns of $U $, $\Sigma_k $ the top $k \times k $ singular values, and $V_k $ the first $k $ columns of $V $.\\

\textbf{The algoritham which i used here is Power iteration method to find eq 2.1.}\\

We need  to find the V as a eigen vectors of$A^TA$ as i told before , for this in this method we will use a trick .\\

we will take random unt vector and continue multiplying it with the $A^TA$
now this random vector can be written interms of sumation of all eigen vectors  in the form $\sigma c_iv_1$ we know eigen vectors when multiplied will not chang the direction it will just extend in that direction .So all the vectors will get extended in that direction proportinal to there eigen values .

\begin{align}
let
\vec{v} = c_1\vec{v_1}+c_2\vec{v_2}+c_3\vec{v_3}+\cdots+c_r\vec{v_r}\\
 A^TA \vec{v} = \lambda_1 c_1 \vec{v}_1 + \lambda_2 c_2 \vec{v}_2 + \lambda_3 c_3 \vec{v}_3 + \lambda_4 c_4 \vec{v}_4 + \cdots + \lambda_r c_r \vec{v}_r
\end{align}
Therefore, the eigen vector corresponding to maximum eigen value will ge extended maximum so there will be more tilting the vector in that direction .\\

Now doing this large number times makes the vector all most in the direction therefore changing into that eigen vector. \\
\begin{align}
  A^TA\vec{v}_1\approx \lambda_1 \vec{v}_1
\end{align}

Also i will add a point , saying that$A^TA$ is symmetric all eigen vector are orthogonal  \\ 

now we to find $u_1$ = just multiply $\vec{v_1}$ with A and then find norm of that vector.
\begin{align}
  A\vec{v}_1\ =  \sigma\vec{u}_1\\
  \vec{u}_1 = \vec{u}_1 /||\vec{u}_1||
\end{align}
$\sigma_1$ value wil  be equal to norm of $u_1$ if u remember sir's video 
$Av= \sigma u$
  \begin{align}
    \sigma_1 = ||\vec{u}_1||\\
  A_1 = u_1\sigma_1 v_1^T
\end{align}


remember to subtract A for each k  from original matrix so that after in next time you do iteration u will get next major eigen vector.
\begin{align}
    A new = A - u_k\sigma_k v_k^T
\end{align}
to find final matrix we just do the formala
\begin{align}
    A_k = U_k \Sigma_k V_k^T
\end{align}
where k is the number of eigen vectors you choose .
\newpage

\section{Pseudocode}
\begin{lstlisting}[language=Python, caption={Pseudocode for SVD-based Image Compression}]
Input: Image matrix A,output matrix B ,row n, columns m,iteraations T,rank k
Output: void

1. crearting a loop to get k  vectors of V.
2.taking a random vector (each element less then one)
3.creating a loop for iteration.
4.inside the loop do A^TAv and divide by its norm and continue.
5.once u get v, do Av and divide by its norm to u.
6.the norm is the sigma for that vectors.
7.to find matrix A_new for this eigen vector we do u*sigma*v^t
8.now,subtract A_new from A to find the next vector
9 this A_new is only B.
      
\end{lstlisting}

% =======================================================
\chapter{Algorithm Comparison and Choice}
There are several approaches for solving for Svd:
\begin{enumerate}
    \item \textbf{jacobiSVD):} 
    this procees was trying to mutiply a rotation matrix with main matrix A and and diagonalize it this basically compression the whole information along the diagonal and also mutliplying the these rotation matrix with matrix V and U and therefore bringing it in $UAV^T$ \\
    why i didnt ch0ose ;\\
    1.it is not good for large matrixes \\
    2.i didnt understand it properly\\
    \item \textbf{Divide ad conquer Svd} 
    this method include dividing the main matrix into  @simpler matrix and then find svd for those simplier matrix again\\
    why i didnt choose it because \\
    it was looking complication\\
    and we we ned to store full matrix (since the matrix is big its hard to do it )
    
\end{enumerate}

\noindent
We chose \textbf{Power iteration } because:
\begin{itemize}
    \item the proces is very easy and simple
    \item her we are not calculating the complete matrix just number of vector and singular values we want fior the given image.
    \item very use full for large matrixes since there is no requirement of storing the complete matrix.
    \item since images are very large , doing by thismethod is very fast
\end{itemize}

% =======================================================
\chapter{Reconstructed Images for Different k}
Below are reconstructed images using different values of $k $ of image 1:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\linewidth]{figs/image1.jpg}
    \caption{Main fig}
    \label{fig:placeholder}
\end{figure}
\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.4\linewidth]{figs/image1.jpg_out_5.jpg}
        \caption{k =5}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.4\linewidth]{figs/image1.jpg_out_20.jpg}
        \caption{k =20}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}


\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.4\linewidth]{figs/image1.jpg_out_50.jpg}
        \caption{k =50}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.4\linewidth]{figs/image1.jpg_out_100.jpg}
        \caption{k =100}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}
Below are reconstructed images using different values of $k $ of image 2:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/image2.jpg}
    \caption{Main fig}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image2.jpg_out_5.jpg}
        \caption{k =5}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image2.jpg_out_20.jpg}
        \caption{k =20}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}

\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image2.jpg_out_50.jpg}
        \caption{k =50}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image2.jpg_out_100.jpg}
        \caption{k =100}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}
\newpage
Below are reconstructed images using different values of $k $ of image 3:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/image3.jpg}
    \caption{Main fig}
    \label{fig:placeholder}
\end{figure}
\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image3.jpg_out_5.jpg}
        \caption{k =5}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image3.jpg_out_20.jpg}
        \caption{k =20}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}

\textbf{\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image3.jpg_out_50.jpg}
        \caption{k =50}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{figs/image3.jpg_out_100.jpg}
        \caption{k =100}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{}
    \label{fig:bothfigs}
\end{figure}
}
% =======================================================
\chapter{Error Analysis}
We measure error using the Frobenius norm:
\begin{align}
E(k) = ||A - A_k||_F
\end{align}
This measures how close the compressed image $A_k $ is to the original $A $.

\input{tables/error_image1}
\input{tables/error_image2}
\newpage
\input{tables/error_image3}

% =======================================================
\chapter{Discussion and Reflections}
\begin{enumerate}
the whole project was to use svd to compression .this provides a balance between storage efficiency and quality.Now lets talk on this\\

\item Trade-off: \\
1.when less number of k values are used ,its results in lesser storage space ,so image will be more compressed but at the cost of image quality.\\
2 when number of k increases quality of image , but the image compression deceases so storage occupied by the image increases .
3.i also want add n computation efficiency , since i am useing power iteration method ,it need not to store tyhe entire storage of image it only takes the singular values that has a greater part of contribution to image making image clear and light.
\item  implementstion : i am implementing the code via power iteration code ,I have alsready told the procedure of  the implementation , i would just add one thing that we incraes the iteration the efficiency of this procedure increases by a great extent  .therefore making image light aand clear.

Overall, this project demonstrates the power of linear algebra in practical image processing applications.

\end{enumerate}
\end{document}
